{
    "env": {
        "inputdir": "/home/dave/Downloads/tmp0FLDyc/input", 
        "outputdir": "/home/dave/Downloads/tmp0FLDyc/output", 
        "scriptdir": "/home/dave/Downloads/tmp0FLDyc/script", 
        "workdir": "/home/dave/Downloads/tmp0FLDyc"
    }, 
    "params": {
        "data_split": 100, 
        "do_full_models": true, 
        "environmental_datasets": [
            {
                "downloadurl": "https://swift.rc.nectar.org.au:8888/v1/AUTH_0bc40c2c2ff94a0b9404e6f960ae5677/multi_res_ridge_top_flat/multi_res_ridge_top_flat.zip", 
                "filename": "/home/dave/Downloads/tmp0FLDyc/input/1c1e196b0b0c4298816886a143d1cc9f/multi_res_ridge_top_flat/data/multi_res_ridge_top_flat.tif", 
                "internalurl": "https://swift.rc.nectar.org.au:8888/v1/AUTH_0bc40c2c2ff94a0b9404e6f960ae5677/multi_res_ridge_top_flat/multi_res_ridge_top_flat.zip", 
                "layer": "http://namespaces.bccvl.org.au/layer#multi_res_ridge_top_flat", 
                "type": "discrete", 
                "uuid": "1c1e196b0b0c4298816886a143d1cc9f", 
                "zippath": "multi_res_ridge_top_flat/data/multi_res_ridge_top_flat.tif"
            },
            {
                "downloadurl": "https://swift.rc.nectar.org.au:8888/v1/AUTH_0bc40c2c2ff94a0b9404e6f960ae5677/multi_res_ridge_top_flat/multi_res_ridge_top_flat2.zip", 
                "filename": "/home/dave/Downloads/tmp0FLDyc/input/1c1e196b0b0c4298816886a143d1cc9f/multi_res_ridge_top_flat/data/multi_res_ridge_top_flat2.tif", 
                "internalurl": "https://swift.rc.nectar.org.au:8888/v1/AUTH_0bc40c2c2ff94a0b9404e6f960ae5677/multi_res_ridge_top_flat/multi_res_ridge_top_flat2.zip", 
                "layer": "http://namespaces.bccvl.org.au/layer#multi_res_ridge_top_flat2", 
                "type": "discrete", 
                "uuid": "1c1e196b0b0c4298816886a143d1cc9f", 
                "zippath": "multi_res_ridge_top_flat/data/multi_res_ridge_top_flat2.tif"
            }

        ], 
        "function": "ann", 
        "maxit": 100, 
        "modeling_id": "bccvl", 
        "nb_run_eval": 10, 
        "nbcv": 5, 
        "prevalence": null, 
        "rang": 0.1, 
        "rescale_all_models": false, 
        "resolution": "http://namespaces.bccvl.org.au/vocab#Resolution3s", 
        "selected_models": "all", 
        "species_absence_dataset": null, 
        "species_number_pseudo_absence_points": 10000, 
        "species_occurrence_dataset": {
            "downloadurl": "https://demo.bccvl.org.au/_debug/bccvl/datasets/species/org-bccvl-content-dataset-1415535127.75/@@download/file/urn:lsid:biodiversity.org.au:afd.taxon:3683c25d-8da5-4002-8bb0-1696668fec8d.csv", 
            "filename": "/home/dave/Downloads/tmp0FLDyc/input/a4ea0a9434884d38aa43f454117cd5fa/urn:lsid:biodiversity.org.au:afd.taxon:3683c25d-8da5-4002-8bb0-1696668fec8d.csv", 
            "internalurl": "http://127.0.0.1:8201/bccvl/datasets/species/org-bccvl-content-dataset-1415535127.75/@@download/file/urn:lsid:biodiversity.org.au:afd.taxon:3683c25d-8da5-4002-8bb0-1696668fec8d.csv", 
            "species": "Sus.scrofa", 
            "uuid": "a4ea0a9434884d38aa43f454117cd5fa"
        }, 
        "species_pseudo_absence_points": true, 
        "tails": "both", 
        "var_import": 0
    }, 
    "result": {
        "outputs": {
            "archives": {
                "model.object.RData.zip": {
                    "files": [
                        "model.object.RData", 
                        "*/*.bccvl.models.out", 
                        "*/.BIOMOD_DATA/bccvl/*", 
                        "*/models/bccvl/*", 
                        "*/proj_current/*.current.projection.out"
                    ], 
                    "genre": "DataGenreSDMModel", 
                    "mimetype": "application/zip", 
                    "title": "R SDM Model object"
                }
            }, 
            "files": {
                "*.Rout": {
                    "genre": "DataGenreLog", 
                    "mimetype": "text/x-r-transcript", 
                    "title": "Log file"
                }, 
                "*.csv": {
                    "genre": "DataGenreSDMEval", 
                    "mimetype": "text/csv", 
                    "title": "Model Evaluation"
                }, 
                "*/proj_current/proj_current_*.tif": {
                    "genre": "DataGenreCP", 
                    "mimetype": "image/geotiff", 
                    "title": "Projection to current"
                }, 
                "*/proj_current/proj_current_ClampingMask.tif": {
                    "genre": "DataGenreClampingMask", 
                    "mimetype": "image/geotiff", 
                    "title": "Clamping Mask"
                }, 
                "mean_response_curves*.png": {
                    "genre": "DataGenreSDMEval", 
                    "mimetype": "image/png", 
                    "title": "Mean Response Curves"
                }, 
                "pROC*.png": {
                    "genre": "DataGenreSDMEval", 
                    "mimetype": "image/png", 
                    "title": "ROC curve"
                }
            }
        }, 
        "results_dir": "/tmp/tmpCUf7LM"
    }, 
    "worker": {
        "files": [
            "species_occurrence_dataset", 
            "species_absence_dataset", 
            "environmental_datasets"
        ], 
        "script": {
            "name": "ann.R", 
            "script": "# FIXME: R env setup should be done on compute host\n#        - lib dir: get rid of it\n#        - don't do install.packages?\n#        -\n# setup R environment\n#if (!file.exists(Sys.getenv(\"R_LIBS_USER\"))) {\n#    dir.create(Sys.getenv(\"R_LIBS_USER\"), recursive=TRUE);\n#}\n#.libPaths(Sys.getenv(\"R_LIBS_USER\"))\n# set CRAN mirror in case we need to download something\n\n## TODO: setup CRAN mirror in .Renviron\n## see http://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html\n## don't install here... just require\n\nr <- getOption(\"repos\")\nr[\"CRAN\"] <- \"http://cran.ms.unimelb.edu.au/\"\noptions(repos=r)\n# print warnings immediately\noptions(warn=1)\n\n\n#script to run to develop distribution models\n###check if libraries are installed, install if necessary and then load them\nnecessary=c(\"ggplot2\",\"tools\", \"rjson\", \"dismo\",\"SDMTools\", \"gbm\", \"rgdal\", \"pROC\", \"R2HTML\", \"png\", \"biomod2\") #list the libraries needed\ninstalled = necessary %in% installed.packages() #check if library is installed\nif (length(necessary[!installed]) >=1) {\n    install.packages(necessary[!installed], dep = T) #if library is not installed, install it\n}\nfor (lib in necessary) {\n    library(lib,character.only=T) #load the libraries\n}\n\n# load parameters\nparams = rjson::fromJSON(file=\"params.json\")\nbccvl.params <- params$params\nbccvl.env <- params$env\nrm(params)\n# set working directory (script runner takes care of it)\nsetwd(bccvl.env$outputdir)\n# Set raster tmpdir - we do this because raster sometimes makes \n# temp files (e.g. when cropping).\n# Might want to make this configurable - e.g. we might want to \n# control maxmemory, and/or other raster options\nrasterOptions(tmpdir=paste(bccvl.env$workdir,\"raster_tmp\",sep=\"/\"))\n\n############################################################\n#\n# define helper functions to use in bccvl\n#\n############################################################\n\n## Needed for tryCatch'ing:\nbccvl.err.null <- function (e) return(NULL)\n\n# read species presence/absence data\n#    return NULL if filename is not  given\n# TODO: shall we set projection here as well? use SpatialPoints?\nbccvl.species.read <- function(filename) {\n    if (!is.null(filename)) {\n        # We might loose precision of lon/lat when ronverting to double,\n        # However, given the nature of the numbers, and the resolution of raster files\n        # we deal with, this shouldn't be a problem.\n        return (read.csv(filename, colClasses=c(\"lon\"=\"numeric\", \"lat\"=\"numeric\")))\n    }\n}\n\n# use either absen.data (1) or generate random pseudo absence points (2)\n# (1) extract \"lat\" and \"lon\" from absen.data\n# (2) generate number of absence points in area of climate.data\nbccvl.dismo.absence <- function(absen.data=NULL,\n                                pseudo.absen.enabled=FALSE,\n                                pseudo.absen.points=0,\n                                climate.data=NULL,\n                                occur.data=NULL) {\n    # TODO: combine random and given absence points:\n    # rbind(absen.datafromfile, bkgd.datarandom)\n    if (pseudo.absen.enabled) {\n        # generate randomPoints\n        bkgd = randomPoints(\n            climate.data,\n            pseudo.absen.points,\n            occur.data)\n        # as data frame\n        absen = as.data.frame(bkgd)\n        # rename columns\n        names(absen) <- c(\"lon\",\"lat\")\n    } else {\n        # otherwise read absence ponits from file\n        absen = bccvl.species.read(absen.data) #read in the background position data lon.lat\n        # keep only lon and lat columns\n        absen = absen[c(\"lon\",\"lat\")]\n    }\n    return(absen);\n}\n\n# warning was doing odd things. I just want to print the deng thing. \nbccvl.log.warning <-function(str, prefix=\"BCCVL Warning: \")\n{\n    print(paste(prefix, str, sep=\"\"))\n}\n\n\n# rasters: a vector of rasters\nbccvl.raster.common.extent <- function(rasters)\n{    \n    extent.list=lapply(rasters, extent)\n    \n    common.extent=extent.list[[1]]\n    equal.extents=TRUE\n    i=2\n    while(i<=length(extent.list))\n    {\n        rast.extent=extent.list[[i]]\n        if (rast.extent != common.extent) equal.extents = FALSE\n        common.extent=intersect(common.extent, rast.extent)\n        i=i+1\n    }\n\n    return (list(equal.extents=equal.extents, common.extent=common.extent))\n}\n\n# rasters: a vector of rasters\nbccvl.raster.lowest.resolution <- function(rasters) \n{\n    res.list = lapply(rasters, res)\n\n    lowest.res = res.list[[1]]\n    index = 1\n\ti = 2\n    while(i<=length(res.list))\n    {\n        # non equal resolutions in the x and y directions is illegal\n        # otherwise the code is flawed\n        # we should put a test here\n        if ( res.list[[i]][1] > lowest.res[[1]] )\n        {\n            lowest.res = res.list[[i]]\n            index = i\n        }\n\t\ti = i+1\n    }\n\n    is.same.res=c()\n    for(i in 1:length(res.list))\n    {\n        is.same.res=rbind(is.same.res, res.list[[i]][1] == lowest.res[[1]])\n    }\n    \n    return (list(lowest.res=lowest.res, index=index, is.same.res=is.same.res))\n}\n\nbccvl.raster.extent.to.str <- function(ext)\n{\n    return(sprintf(\"xmin=%f xmax=%f ymin=%f ymax=%f\", ext@xmin, ext@xmax, ext@ymin, ext@ymax));\n}\n\n\n# raster.filenames : a vector of filenames that will be loaded as rasters \nbccvl.rasters.to.common.extent.and.lowest.resolution <- function(raster.filenames)\n{\n    rasters = lapply(raster.filenames, raster)\n\n    ce = bccvl.raster.common.extent(rasters)  \n    if (ce$equal.extents == FALSE)\n    {\n        bccvl.log.warning(sprintf(\"Auto cropped to common extent %s\", bccvl.raster.extent.to.str(ce$common.extent)))\n        crop_common <- function(x) { crop(raster(x), ce$common.extent) }\n        rasters = lapply(raster.filenames, crop_common)\n    }\n\n    lr=bccvl.raster.lowest.resolution(rasters)\n    if (sum(lr$is.same.res == FALSE) != 0)\n    {\n        bccvl.log.warning(sprintf(\"Auto resampled to lowest resolution [%f %f]\", lr$lowest.res[[1]], lr$lowest.res[[2]]))\n    }\n\n    master=rasters[[lr$index]] \n    index=1\n    resamp_func <- function(x) \n    { \n        rsp = if (lr$is.same.res[index]) x else resample(x, master)\n        index <<- index + 1\n        return(rsp)\n    }\n\n    return(lapply(rasters, resamp_func))\n\n}\n\n# return a RasterStack of given vector of input files\n# intersecting extent\n# lowest resolution\nbccvl.enviro.stack <- function(filenames) {\n    \n    rasters = bccvl.rasters.to.common.extent.and.lowest.resolution(filenames)\n    return(stack(rasters))\n}\n\n# function to save projection output raster\nbccvl.saveModelProjection <- function(model.obj, projection.name, species, outputdir=bccvl.env$outputdir) {\n    ## save projections under biomod2 compatible name:\n    ##  proj_name_species.tif\n    ##  only useful for dismo outputs\n    basename = paste(\"proj\", projection.name, species, sep=\"_\")\n    filename = file.path(outputdir, paste(basename, 'tif', sep=\".\"))\n    writeRaster(model.obj, filename, format=\"GTiff\", options=\"COMPRESS=LZW\", overwrite=TRUE)\n}\n\n# function to save RData in outputdir\nbccvl.save <- function(robj, name, outputdir=bccvl.env$outputdir) {\n    filename = file.path(outputdir, name)\n    save(robj, file=filename)\n}\n\n# function to save CSV Data in outputdir\nbccvl.write.csv <- function(robj, name, outputdir=bccvl.env$outputdir) {\n    filename = file.path(outputdir, name)\n    write.csv(robj, file=filename)\n}\n\n# function to get model object\nbccvl.getModelObject <- function(model.file=bccvl.env$inputmodel) {\n    return (get(load(file=model.file)))\n}\n\n# convert all .gri/.grd found in folder to gtiff\n# TODO: extend to handle other grid file formats, e.g. .asc\nbccvl.grdtogtiff <- function(folder) {\n    grdfiles <- list.files(path=folder,\n                           pattern=\"^.*\\\\.gri\")\n    for (grdfile in grdfiles) {\n        # get grid file name\n        grdname <- file_path_sans_ext(grdfile)\n        # read grid raster\n        grd <- raster(file.path(folder, grdfile))\n        # write raster as geotiff\n        filename = file.path(folder, paste(grdname, 'tif', sep=\".\"))\n        writeRaster(grd, filename, format=\"GTiff\", options=\"COMPRESS=LZW\", overwrite=TRUE)\n        # remove grd files\n        file.remove(file.path(folder, paste(grdname, c(\"grd\",\"gri\"), sep=\".\")))\n    }\n}\n\n############################################################\n#\n# define helper functions for projections\n#\n############################################################\n\n# function to check that the environmental layers used to project the\n# model are the same as the ones used to create the model object\n#    model.obj     ... model to project\n#    climatelayers ... climate data to project onto\nbccvl.checkModelLayers <- function(model.obj, climatelayers) {\n    message(\"Checking environmental layers used for projection\")\n    # get the names of the environmental layers from the original model\n    if (inherits(model.obj, \"DistModel\")) {\n        # dismo package\n        model.layers = colnames(model.obj@presence)\n    } else if (inherits(model.obj, \"gbm\")) {\n        # brt package\n        model.layers = summary(model.obj)$var\n    } else if (inherits(model.obj, \"BIOMOD.models.out\")) {\n        # biomod package\n        model.layers = model.obj@expl.var.names\n    }\n\n    # get the names of the climate scenario's env layers\n    pred.layers = names(climatelayers)\n\n    # check if the env layers were in the original model\n    if(sum(!(pred.layers %in% model.layers)) > 0 ){\n        message(\"Dropping environmental layers not used in the original model creation...\")\n        # create a new list of env predictors by dropping layers not in the original model\n        new.predictors = climatelayers\n        for (pl in pred.layers) {\n            if (!(pl %in% model.layers)) {\n                new.predictors = dropLayer(new.predictors, pl)\n            }\n        }\n        return(new.predictors)\n    } else {\n        return(climatelayers)\n    }\n}\n\n\n############################################################\n#\n# patched dismo:gbm.step function\n# \n# patched to work with 1D input feature data. Calls dismo:::.roc and dismo:::.calibration\n#\n# patched from dismo version 0.9.4\n#\n# requires that dismo is already loaded\n#\n############################################################\n#\n# j. leathwick/j. elith - 19th September 2005\n#\n# version 2.9\n#\n# function to assess optimal no of boosting trees using k-fold cross validation\n#\n# implements the cross-validation procedure described on page 215 of\n# Hastie T, Tibshirani R, Friedman JH (2001) The Elements of Statistical Learning:\n# Data Mining, Inference, and Prediction Springer-Verlag, New York.\n#\n# divides the data into 10 subsets, with stratification by prevalence if required for pa data\n# then fits a gbm model of increasing complexity along the sequence from n.trees to n.trees + (n.steps * step.size)\n# calculating the residual deviance at each step along the way\n# after each fold processed, calculates the average holdout residual deviance and its standard error\n# then identifies the optimal number of trees as that at which the holdout deviance is minimised\n# and fits a model with this number of trees, returning it as a gbm model along with additional information\n# from the cv selection process\n#\n# updated 13/6/05 to accommodate weighting of sites\n#\n# updated 19/8/05 to increment all folds simultaneously, allowing the stopping rule\n# for the maxinum number of trees to be fitted to be imposed by the data,\n# rather than being fixed in advance\n#\n# updated 29/8/05 to return cv test statistics, and deviance as mean\n# time for analysis also returned via unclass(Sys.time())\n#\n# updated 5/9/05 to use external function calc.deviance\n# and to return cv test stats via predictions formed from fold models\n# with n.trees = target.trees\n#\n# updated 15/5/06 to calculate variance of fitted and predicted values across folds\n# these can be expected to approximate the variance of fitted values\n# as would be estimated for example by bootstrapping\n# as these will underestimate the true variance\n# they are corrected by multiplying by (n-1)2/n\n# where n is the number of folds\n#\n# updated 25/3/07 tp allow varying of bag fraction\n#\n# requires gbm library from Cran\n# requires roc and calibration scripts of J Elith\n# requires calc.deviance script of J Elith/J Leathwick\n#\n#\ngbm.step.masked.data.frame <-\nfunction(data,\n         mask)\n{\n   if (length(names(data)) == 1 ) {\n      ss <- data.frame(data[mask,])\n   } else {\n\t  ss <- data[mask,]\n   }\n   return(ss);\n}\n\ngbm.step <-\nfunction (data,                             # the input dataframe\n  gbm.x,                                    # the predictors\n  gbm.y,                                    # and response\n  offset = NULL,                            # allows an offset to be specified\n  fold.vector = NULL,                       # allows a fold vector to be read in for CV with offsets,\n  tree.complexity = 1,                      # sets the complexity of individual trees\n  learning.rate = 0.01,                     # sets the weight applied to inidivudal trees\n  bag.fraction = 0.75,                      # sets the proportion of observations used in selecting variables\n  site.weights = rep(1, nrow(data)),        # allows varying weighting for sites\n  var.monotone = rep(0, length(gbm.x)),     # restricts responses to individual predictors to monotone\n  n.folds = 10,                             # number of folds\n  prev.stratify = TRUE,                     # prevalence stratify the folds - only for p/a data\n  family = \"bernoulli\",                     # family - bernoulli (=binomial), poisson, laplace or gaussian\n  n.trees = 50,                             # number of initial trees to fit\n  step.size = n.trees,                      # numbers of trees to add at each cycle\n  max.trees = 10000,                        # max number of trees to fit before stopping\n  tolerance.method = \"auto\",                # method to use in deciding to stop - \"fixed\" or \"auto\"\n  tolerance = 0.001,                        # tolerance value to use - if method == fixed is absolute,\n                                            # if auto is multiplier * total mean deviance\n  keep.data = FALSE,                        # keep raw data in final model\n  plot.main = TRUE,                         # plot hold-out deviance curve\n  plot.folds = FALSE,                       # plot the individual folds as well\n  verbose = TRUE,                           # control amount of screen reporting\n  silent = FALSE,                           # to allow running with no output for simplifying model)\n  keep.fold.models = FALSE,                 # keep the fold models from cross valiation\n  keep.fold.vector = FALSE,                 # allows the vector defining fold membership to be kept\n  keep.fold.fit = FALSE,                    # allows the predicted values for observations from CV to be kept\n  ...)                                      # allows for any additional plotting parameters\n{\n\n  if (! require(gbm) ) { stop ('you need to install the gbm package to run this function') }\n\n  if (silent) verbose <- FALSE\n\n# initiate timing call\n\n  z1 <- unclass(Sys.time())\n\n# setup input data and assign to position one\n\n  dataframe.name <- deparse(substitute(data))   # get the dataframe name\n\n  data <- eval(data)\n  # TODO get to the bottom of why this is no good when input data are 1D\n  #x.data <- eval(data[, gbm.x])                 #form the temporary datasets\n  x.data <- data[gbm.x]                 #form the temporary datasets\n  names(x.data) <- names(data)[gbm.x]\n  y.data <- eval(data[, gbm.y])\n  sp.name <- names(data)[gbm.y]\n  if (family == \"bernoulli\") prevalence <- mean(y.data)\n\n#  assign(\"x.data\", x.data, env = globalenv())               #and assign them for later use\n#  assign(\"y.data\", y.data, env = globalenv())\n\n  offset.name <- deparse(substitute(offset))   # get the dataframe name\n  offset = eval(offset)\n\n  n.cases <- nrow(data)\n  n.preds <- length(gbm.x)\n\n  if (!silent) {\n    cat(\"\\n\",\"\\n\",\"GBM STEP - version 2.9\",\"\\n\",\"\\n\")\n    cat(\"Performing cross-validation optimisation of a boosted regression tree model \\n\")\n    cat(\"for\",sp.name,\"with dataframe\",dataframe.name,\"and using a family of\",family,\"\\n\")\n    cat(\"Using\",n.cases,\"observations and\",n.preds,\"predictors \\n\")\n  }\n\n# set up the selector variable either with or without prevalence stratification\n\n  if (is.null(fold.vector)) {\n\n    if (prev.stratify & family == \"bernoulli\") {\n      presence.mask <- data[,gbm.y] == 1\n      absence.mask <- data[,gbm.y] == 0\n      n.pres <- sum(presence.mask)\n      n.abs <- sum(absence.mask)\n\n# create a vector of randomised numbers and feed into presences\n      selector <- rep(0,n.cases)\n      temp <- rep(seq(1, n.folds, by = 1), length = n.pres)\n      temp <- temp[order(runif(n.pres, 1, 100))]\n      selector[presence.mask] <- temp\n\n# and then do the same for absences\n      temp <- rep(seq(1, n.folds, by = 1), length = n.abs)\n      temp <- temp[order(runif(n.abs, 1, 100))]\n      selector[absence.mask] <- temp\n      }\n\n    else {  #otherwise make them random with respect to presence/absence\n      selector <- rep(seq(1, n.folds, by = 1), length = n.cases)\n      selector <- selector[order(runif(n.cases, 1, 100))]\n      }\n    }\n  else {\n    if (length(fold.vector) != n.cases) stop(\"supplied fold vector is of wrong length\")\n    cat(\"loading user-supplied fold vector \\n\")\n    selector <- eval(fold.vector)\n    }\n\n# set up the storage space for results\n\n  pred.values <- rep(0, n.cases)\n\n  cv.loss.matrix <- matrix(0, nrow = n.folds, ncol = 1)\n  training.loss.matrix <- matrix(0, nrow = n.folds, ncol = 1)\n  trees.fitted <- n.trees\n\n  model.list <- list(paste(\"model\",c(1:n.folds),sep=\"\"))     # dummy list for the tree models\n\n# set up the initial call to gbm\n\n  if (is.null(offset)) {\n    gbm.call <- paste(\"gbm(y.subset ~ .,data=x.subset, n.trees = n.trees,\n    interaction.depth = tree.complexity, shrinkage = learning.rate,\n    bag.fraction = bag.fraction, weights = weight.subset,\n    distribution = as.character(family), var.monotone = var.monotone,\n    verbose = FALSE)\", sep=\"\")\n    }\n  else {\n    gbm.call <- paste(\"gbm(y.subset ~ . + offset(offset.subset),\n    data=x.subset, n.trees = n.trees,\n    interaction.depth = tree.complexity, shrinkage = learning.rate,\n    bag.fraction = bag.fraction, weights = weight.subset,\n    distribution = as.character(family), var.monotone = var.monotone,\n    verbose = FALSE)\", sep=\"\")\n    }\n\n  n.fitted <- n.trees\n\n# calculate the total deviance\n\n  y_i <- y.data\n\n  u_i <- sum(y.data * site.weights) / sum(site.weights)\n  u_i <- rep(u_i,length(y_i))\n\n  total.deviance <- calc.deviance(y_i, u_i, weights = site.weights, family = family, calc.mean = FALSE)\n\n  mean.total.deviance <- total.deviance/n.cases\n\n  tolerance.test <- tolerance\n\n  if (tolerance.method == \"auto\") {\n     tolerance.test <- mean.total.deviance * tolerance\n  }\n\n# now step through the folds setting up the initial call\n\n\tif (!silent){\n\t\tcat(\"creating\",n.folds,\"initial models of\",n.trees,\"trees\",\"\\n\")\n\t\tif (prev.stratify & family == \"bernoulli\") cat(\"\\n\",\"folds are stratified by prevalence\",\"\\n\")\n\t\telse cat(\"\\n\",\"folds are unstratified\",\"\\n\")\n\t\tcat (\"total mean deviance = \",round(mean.total.deviance,4),\"\\n\")\n\t\tcat(\"tolerance is fixed at \",round(tolerance.test,4),\"\\n\")\n\t\tif (tolerance.method != \"fixed\" & tolerance.method != \"auto\") {\n\t\t\tcat(\"invalid argument for tolerance method - should be auto or fixed\",\"\\n\")\n\t\t\treturn()\n\t\t}\n\t}\n\n\tif (verbose) cat(\"ntrees resid. dev.\",\"\\n\")\n\n\tfor (i in 1:n.folds) {\n\n\t\tmodel.mask <- selector != i  #used to fit model on majority of data\n\t\tpred.mask <- selector == i   #used to identify the with-held subset\n\n\t\ty.subset <- y.data[model.mask]\n        x.subset = gbm.step.masked.data.frame(x.data, model.mask)\n\n\t\tweight.subset <- site.weights[model.mask]\n\n\t\tif (!is.null(offset)) {\n\t\t\toffset.subset <- offset[model.mask]\n\t\t} else {\n\t\t\toffset.subset <- NULL\n\t\t}\n\n\t\tmodel.list[[i]] <- eval(parse(text = gbm.call))\n\n\t\tfitted.values <- model.list[[i]]$fit  #predict.gbm(model.list[[i]], x.subset, type = \"response\", n.trees = n.trees)\n\t\tif (!is.null(offset)) fitted.values <- fitted.values + offset[model.mask]\n\t\tif (family == \"bernoulli\") fitted.values <- exp(fitted.values)/(1 + exp(fitted.values))\n\t\tif (family == \"poisson\") fitted.values <- exp(fitted.values)\n\n        x.pred.subset = gbm.step.masked.data.frame(x.data, pred.mask)\n\n\t\t#pred.values[pred.mask] <- predict.gbm(model.list[[i]], x.data[pred.mask, ], n.trees = n.trees)\n\t\tpred.values[pred.mask] <- predict.gbm(model.list[[i]], x.pred.subset, n.trees = n.trees)\n\t\tif (!is.null(offset)) pred.values[pred.mask] <- pred.values[pred.mask] + offset[pred.mask]\n\t\tif (family == \"bernoulli\") pred.values[pred.mask] <- exp(pred.values[pred.mask])/(1 + exp(pred.values[pred.mask]))\n\t\tif (family == \"poisson\") pred.values[pred.mask] <- exp(pred.values[pred.mask])\n\n# calc training deviance\n\n\t\ty_i <- y.subset\n\t\tu_i <- fitted.values\n\t\tweight.fitted <- site.weights[model.mask]\n\t\ttraining.loss.matrix[i,1] <- calc.deviance(y_i, u_i, weight.fitted, family = family)\n\n# calc holdout deviance\n\n\t\ty_i <- y.data[pred.mask]\n\t\tu_i <- pred.values[pred.mask]\n\t\tweight.preds <- site.weights[pred.mask]\n\t\tcv.loss.matrix[i,1] <- calc.deviance(y_i, u_i, weight.preds, family = family)\n\n\t} # end of first loop\n\n# now process until the change in mean deviance is =< tolerance or max.trees is exceeded\n\n\tdelta.deviance <- 1\n\n\tcv.loss.values <- apply(cv.loss.matrix,2,mean)\n\tif (verbose) cat(n.fitted,\"  \",round(cv.loss.values,4),\"\\n\")\n\n\tif (!silent) cat(\"now adding trees...\",\"\\n\")\n\n\tj <- 1\n\n\twhile (delta.deviance > tolerance.test & n.fitted < max.trees) {  # beginning of inner loop\n\n# add a new column to the results matrice..\n\n\t\ttraining.loss.matrix <- cbind(training.loss.matrix,rep(0,n.folds))\n\t\tcv.loss.matrix <- cbind(cv.loss.matrix,rep(0,n.folds))\n\n\t\tn.fitted <- n.fitted + step.size\n\t\ttrees.fitted <- c(trees.fitted,n.fitted)\n\n\t\tj <- j + 1\n\n\t\tfor (i in 1:n.folds) {\n\n\t\t\tmodel.mask <- selector != i  #used to fit model on majority of data\n\t\t\tpred.mask <- selector == i   #used to identify the with-held subset\n\n\t\t\ty.subset <- y.data[model.mask]\n\n            x.subset = gbm.step.masked.data.frame(x.data, model.mask)\n\t\t\tweight.subset <- site.weights[model.mask]\n\t\t\tif (!is.null(offset)) {\n\t\t\t\toffset.subset <- offset[model.mask]\n\t\t\t}\n\t\t\tmodel.list[[i]] <- gbm.more(model.list[[i]], weights = weight.subset, step.size)\n\n\t\t\tfitted.values <- model.list[[i]]$fit # predict.gbm(model.list[[i]],x.subset, type = \"response\", n.trees = n.fitted)\n\t\t\tif (!is.null(offset)) fitted.values <- fitted.values + offset[model.mask]\n\t\t\tif (family == \"bernoulli\") fitted.values <- exp(fitted.values)/(1 + exp(fitted.values))\n\t\t\tif (family == \"poisson\") fitted.values <- exp(fitted.values)\n\n            x.pred.subset = gbm.step.masked.data.frame(x.data, pred.mask)\n\n\t\t\t#pred.values[pred.mask] <- predict.gbm(model.list[[i]], x.data[pred.mask, ], n.trees = n.fitted)\n\t\t\tpred.values[pred.mask] <- predict.gbm(model.list[[i]], x.pred.subset, n.trees = n.fitted)\n\t\t\tif (!is.null(offset)) pred.values[pred.mask] <- pred.values[pred.mask] + offset[pred.mask]\n\t\t\tif (family == \"bernoulli\") pred.values[pred.mask] <- exp(pred.values[pred.mask])/(1 + exp(pred.values[pred.mask]))\n\t\t\tif (family == \"poisson\") pred.values[pred.mask] <- exp(pred.values[pred.mask])\n\n# calculate training deviance\n\n      y_i <- y.subset\n      u_i <- fitted.values\n      weight.fitted <- site.weights[model.mask]\n      training.loss.matrix[i,j] <- calc.deviance(y_i, u_i, weight.fitted, family = family)\n\n# calc holdout deviance\n\n      u_i <- pred.values[pred.mask]\n      y_i <- y.data[pred.mask]\n      weight.preds <- site.weights[pred.mask]\n      cv.loss.matrix[i,j] <- calc.deviance(y_i, u_i, weight.preds, family = family)\n\n    }  # end of inner loop\n\n    cv.loss.values <- apply(cv.loss.matrix,2,mean)\n\n    if (j < 5) {\n      if (cv.loss.values[j] > cv.loss.values[j-1]) {\n        if (!silent) cat(\"restart model with a smaller learning rate or smaller step size...\")\n        return()\n      }\n    }\n\n      if (j >= 20) {   #calculate stopping rule value\n        test1 <- mean(cv.loss.values[(j-9):j])\n        test2 <- mean(cv.loss.values[(j-19):(j-9)])\n        delta.deviance <- test2 - test1\n      }\n\n      if (verbose) cat(n.fitted,\" \",round(cv.loss.values[j],4),\"\\n\")\n\n  } # end of while loop\n\n# now begin process of calculating optimal number of trees\n  training.loss.values <- apply(training.loss.matrix,2,mean)\n\n  cv.loss.ses <- rep(0,length(cv.loss.values))\n  cv.loss.ses <- sqrt(apply(cv.loss.matrix,2,var)) / sqrt(n.folds)\n\n# find the target holdout deviance\n\n  y.bar <- min(cv.loss.values)\n\n# plot out the resulting curve of holdout deviance\n\n  if (plot.main) {\n\n    y.min <- min(cv.loss.values - cv.loss.ses)  #je added multiplier 10/8/05\n    y.max <- max(cv.loss.values + cv.loss.ses)  #je added multiplier 10/8/05 }\n\n    if (plot.folds) {\n      y.min <- min(cv.loss.matrix)\n      y.max <- max(cv.loss.matrix) }\n\n      plot(trees.fitted, cv.loss.values, type = 'l', ylab = \"holdout deviance\", xlab = \"no. of trees\", ylim = c(y.min,y.max), ...)\n      abline(h = y.bar, col = 2)\n\n      lines(trees.fitted, cv.loss.values + cv.loss.ses, lty=2)\n      lines(trees.fitted, cv.loss.values - cv.loss.ses, lty=2)\n\n      if (plot.folds) {\n        for (i in 1:n.folds) {\n          lines(trees.fitted, cv.loss.matrix[i,],lty = 3)\n      }\n    }\n  }\n\n# identify the optimal number of trees\n\n  target.trees <- trees.fitted[match(TRUE,cv.loss.values == y.bar)]\n\n  if(plot.main) {\n    abline(v = target.trees, col=3)\n    title(paste(sp.name,\", d - \",tree.complexity,\", lr - \",learning.rate, sep=\"\"))\n  }\n\n# estimate the cv deviance and test statistics\n# includes estimates of the standard error of the fitted values added 2nd may 2005\n\n  cv.deviance.stats <- rep(0, n.folds)\n  cv.roc.stats <- rep(0, n.folds)\n  cv.cor.stats <- rep(0, n.folds)\n  cv.calibration.stats <- matrix(0, ncol=5, nrow = n.folds)\n  if (family == \"bernoulli\") threshold.stats <- rep(0, n.folds)\n\n  fitted.matrix <- matrix(NA, nrow = n.cases, ncol = n.folds)  # used to calculate se's\n  fold.fit <- rep(0,n.cases)\n\n  for (i in 1:n.folds) {\n\n    pred.mask <- selector == i   #used to identify the with-held subset\n    model.mask <- selector != i  #used to fit model on majority of data\n\n    x.subset = gbm.step.masked.data.frame(x.data, model.mask)\n    #fits <- predict.gbm(model.list[[i]], x.data[model.mask, ], n.trees = target.trees)\n    fits <- predict.gbm(model.list[[i]], x.subset, n.trees = target.trees)\n    if (!is.null(offset)) fits <- fits + offset[model.mask]\n    if (family == \"bernoulli\") fits <- exp(fits)/(1 + exp(fits))\n    if (family == \"poisson\") fits <- exp(fits)\n    fitted.matrix[model.mask,i] <- fits\n\n    x.pred.subset = gbm.step.masked.data.frame(x.data, pred.mask)\n    #fits <- predict.gbm(model.list[[i]], x.data[pred.mask, ], n.trees = target.trees)\n    fits <- predict.gbm(model.list[[i]], x.pred.subset, n.trees = target.trees)\n    if (!is.null(offset)) fits <- fits + offset[pred.mask]\n    fold.fit[pred.mask] <- fits  # store the linear predictor values\n    if (family == \"bernoulli\") fits <- exp(fits)/(1 + exp(fits))\n    if (family == \"poisson\") fits <- exp(fits)\n    fitted.matrix[pred.mask,i] <- fits\n\n    y_i <- y.data[pred.mask]\n    u_i <- fitted.matrix[pred.mask,i]  #pred.values[pred.mask]\n    weight.preds <- site.weights[pred.mask]\n\n    cv.deviance.stats[i] <- calc.deviance(y_i, u_i, weight.preds, family = family)\n\n    cv.cor.stats[i] <- cor(y_i,u_i)\n\n    if (family == \"bernoulli\") {\n      cv.roc.stats[i] <- dismo:::.roc(y_i,u_i)\n      cv.calibration.stats[i,] <- dismo:::.calibration(y_i,u_i,\"binomial\")\n      threshold.stats[i] <- approx(ppoints(u_i), sort(u_i,decreasing = T), prevalence)$y\n    }\n\n    if (family == \"poisson\") {\n      cv.calibration.stats[i,] <- dismo:::.calibration(y_i,u_i,\"poisson\")\n    }\n  }\n\n  fitted.vars <- apply(fitted.matrix,1, var, na.rm = TRUE)\n\n# now calculate the mean and se's for the folds\n\n  cv.dev <- mean(cv.deviance.stats, na.rm = TRUE)\n  cv.dev.se <- sqrt(var(cv.deviance.stats)) / sqrt(n.folds)\n\n  cv.cor <- mean(cv.cor.stats, na.rm = TRUE)\n  cv.cor.se <- sqrt(var(cv.cor.stats, use = \"complete.obs\")) / sqrt(n.folds)\n\n  cv.roc <- 0.0\n  cv.roc.se <- 0.0\n\n  if (family == \"bernoulli\") {\n    cv.roc <- mean(cv.roc.stats,na.rm=TRUE)\n    cv.roc.se <- sqrt(var(cv.roc.stats, use = \"complete.obs\")) / sqrt(n.folds)\n    cv.threshold <- mean(threshold.stats, na.rm = T)\n    cv.threshold.se <- sqrt(var(threshold.stats, use = \"complete.obs\")) / sqrt(n.folds)\n  }\n\n  cv.calibration <- 0.0\n  cv.calibration.se <- 0.0\n\n  if (family == \"poisson\" | family == \"bernoulli\") {\n    cv.calibration <- apply(cv.calibration.stats,2,mean)\n    cv.calibration.se <- apply(cv.calibration.stats,2,var)\n    cv.calibration.se <- sqrt(cv.calibration.se) / sqrt(n.folds) }\n\n# fit the final model\n\n\tif (is.null(offset)) {\n\t\tgbm.call <- paste(\"gbm(y.data ~ .,data=x.data, n.trees = target.trees,\n\t\tinteraction.depth = tree.complexity, shrinkage = learning.rate, bag.fraction = bag.fraction, weights = site.weights,\n\t\tdistribution = as.character(family), var.monotone = var.monotone, verbose = FALSE)\", sep=\"\")\n\t} else {\n\t\tgbm.call <- paste(\"gbm(y.data ~ . + offset(offset),data=x.data, n.trees = target.trees,\n\t\t\tinteraction.depth = tree.complexity, shrinkage = learning.rate, bag.fraction = bag.fraction, weights = site.weights,\n\t\t\tdistribution = as.character(family), var.monotone = var.monotone,  verbose = FALSE)\", sep=\"\")\n    }\n\n  if (!silent) cat(\"fitting final gbm model with a fixed number of \",target.trees,\" trees for \",sp.name,\"\\n\")\n\n  gbm.object <- eval(parse(text = gbm.call))\n\n  best.trees <- target.trees\n\n#extract fitted values and summary table\n\n  gbm.summary <- summary(gbm.object,n.trees = target.trees, plotit = FALSE)\n\n  fits <- predict.gbm(gbm.object,x.data,n.trees = target.trees)\n  if (!is.null(offset)) fits <- fits + offset\n  if (family == \"bernoulli\") fits <- exp(fits)/(1 + exp(fits))\n  if (family == \"poisson\") fits <- exp(fits)\n  fitted.values <- fits\n\n  y_i <- y.data\n  u_i <- fitted.values\n  resid.deviance <- calc.deviance(y_i, u_i, weights = site.weights, family = family, calc.mean = FALSE)\n\n  self.cor <- cor(y_i,u_i)\n  self.calibration <- 0.0\n  self.roc <- 0.0\n\n  if (family == \"bernoulli\") {  # do this manually as we need the residuals\n    deviance.contribs <- (y_i * log(u_i)) + ((1-y_i) * log(1 - u_i))\n    residuals <- sqrt(abs(deviance.contribs * 2))\n    residuals <- ifelse((y_i - u_i) < 0, 0 - residuals, residuals)\n    self.roc <- dismo:::.roc(y_i,u_i)\n    self.calibration <- dismo:::.calibration(y_i,u_i,\"binomial\")\n  }\n\n  if (family == \"poisson\") {   # do this manually as we need the residuals\n    deviance.contribs <- ifelse(y_i == 0, 0, (y_i * log(y_i/u_i))) - (y_i - u_i)\n    residuals <- sqrt(abs(deviance.contribs * 2))\n    residuals <- ifelse((y_i - u_i) < 0, 0 - residuals, residuals)\n    self.calibration <- dismo:::.calibration(y_i,u_i,\"poisson\")\n  }\n\n  if (family == \"gaussian\" | family == \"laplace\") {\n    residuals <- y_i - u_i\n  }\n\n  mean.resid.deviance <- resid.deviance/n.cases\n\n  z2 <- unclass(Sys.time())\n  elapsed.time.minutes <- round((z2 - z1)/ 60,2)  #calculate the total elapsed time\n\n  if (verbose) {\n    cat(\"\\n\")\n    cat(\"mean total deviance =\", round(mean.total.deviance,3),\"\\n\")\n    cat(\"mean residual deviance =\", round(mean.resid.deviance,3),\"\\n\",\"\\n\")\n    cat(\"estimated cv deviance =\", round(cv.dev,3),\"; se =\",\n      round(cv.dev.se,3),\"\\n\",\"\\n\")\n    cat(\"training data correlation =\",round(self.cor,3),\"\\n\")\n    cat(\"cv correlation = \",round(cv.cor,3),\"; se =\",round(cv.cor.se,3),\"\\n\",\"\\n\")\n    if (family == \"bernoulli\") {\n      cat(\"training data ROC score =\",round(self.roc,3),\"\\n\")\n      cat(\"cv ROC score =\",round(cv.roc,3),\"; se =\",round(cv.roc.se,3),\"\\n\",\"\\n\")\n    }\n    cat(\"elapsed time - \",round(elapsed.time.minutes,2),\"minutes\",\"\\n\")\n  }\n\n  if (n.fitted == max.trees & !silent) {\n    cat(\"\\n\",\"########### warning ##########\",\"\\n\",\"\\n\")\n    cat(\"maximum tree limit reached - results may not be optimal\",\"\\n\")\n    cat(\"  - refit with faster learning rate or increase maximum number of trees\",\"\\n\")\n  }\n\n# now assemble data to be returned\n\n  gbm.detail <- list(dataframe = dataframe.name, gbm.x = gbm.x, predictor.names = names(x.data),\n    gbm.y = gbm.y, response.name = sp.name, offset = offset.name, family = family, tree.complexity = tree.complexity,\n    learning.rate = learning.rate, bag.fraction = bag.fraction, cv.folds = n.folds,\n    prev.stratification = prev.stratify, max.fitted = n.fitted, n.trees = target.trees,\n    best.trees = target.trees, train.fraction = 1.0, tolerance.method = tolerance.method,\n    tolerance = tolerance, var.monotone = var.monotone, date = date(),\n    elapsed.time.minutes = elapsed.time.minutes)\n\n  training.stats <- list(null = total.deviance, mean.null = mean.total.deviance,\n    resid = resid.deviance, mean.resid = mean.resid.deviance, correlation = self.cor,\n    discrimination = self.roc, calibration = self.calibration)\n\n  cv.stats <- list(deviance.mean = cv.dev, deviance.se = cv.dev.se,\n    correlation.mean = cv.cor, correlation.se = cv.cor.se,\n    discrimination.mean = cv.roc, discrimination.se = cv.roc.se,\n    calibration.mean = cv.calibration, calibration.se = cv.calibration.se)\n\n  if (family == \"bernoulli\") {\n    cv.stats$cv.threshold <- cv.threshold\n    cv.stats$cv.threshold.se <- cv.threshold.se\n  }\n\n#  rm(x.data,y.data, envir = globalenv())           #finally, clean up the temporary dataframes\n\n# and assemble results for return\n\n  gbm.object$gbm.call <- gbm.detail\n  gbm.object$fitted <- fitted.values\n  gbm.object$fitted.vars <- fitted.vars\n  gbm.object$residuals <- residuals\n  gbm.object$contributions <- gbm.summary\n  gbm.object$self.statistics <- training.stats\n  gbm.object$cv.statistics <- cv.stats\n  gbm.object$weights <- site.weights\n  gbm.object$trees.fitted <- trees.fitted\n  gbm.object$training.loss.values <- training.loss.values\n  gbm.object$cv.values <- cv.loss.values\n  gbm.object$cv.loss.ses <- cv.loss.ses\n  gbm.object$cv.loss.matrix <- cv.loss.matrix\n  gbm.object$cv.roc.matrix <- cv.roc.stats\n\n  if (keep.fold.models) gbm.object$fold.models <- model.list\n  else gbm.object$fold.models <- NULL\n\n  if (keep.fold.vector) gbm.object$fold.vector <- selector\n  else gbm.object$fold.vector <- NULL\n\n  if (keep.fold.fit) gbm.object$fold.fit <- fold.fit\n  else gbm.object$fold.fit <- NULL\n\n  return(gbm.object)\n}\n\n\n\nfamily_from_string <- function(s)\n{\n    # get family from a string (character) in a safe way\n    # works for all variants of the R family object (e.g. see ?family)\n    # i.e.\n    # family_from_string(\"binomial\")\n    # family_from_string(\"binomial(link=logit)\")\n    # family_from_string(\"binomial(link=\\\"logit\\\")\")\n    # ... \n    # family_from_string(\"quasi(link = \\\"identity\\\", variance = \\\"constant\\\")\")\n\n    s=gsub(pattern=\"\\\"|| \", replacement=\"\", s) # strip quotes and spaces \n    f=gsub(pattern=\"\\\\(.*\\\\)\", replacement=\"\", s) # the name of the function\n    \n    allowable= c(\"binomial\",\n                \"gaussian\",\n                \"Gamma\",\n                \"inverse.gaussian\",\n                \"poisson\",\n                \"quasi\",\n                \"quasibinomial\",\n                \"quasipoisson\")\n\n    if (! f %in% allowable )\n    {\n        stop(sprintf(\"unsupported function %s\", f))\n    }\n         \n    fargs=gsub(pattern=\".*\\\\(||\\\\)\", \n               replacement=\"\", \n               sub(pattern=f, \n                    replacement=\"\",\n                    s)) #get the args inside the parentheses\n    args=list()\n\n    if (fargs != \"\")\n    {\n        l=strsplit(fargs, \",\")[[1]]\n        for( i in 1:length(l) )\n        {\n            ll=strsplit(l[i],\"=\")[[1]]\n            if (length(ll) == 2)\n            {\n                args[ll[1]] = ll[2]\n            }\n            else\n            {\n                stop(sprintf(\"unhandled result when splitting %s\", l[i])) \n            }\n        }\n    }\n    return (do.call(what=f, args=args))\n}\n\n\n\n######################################################################################\n# model accuracy helpers\n######################################################################################\n\n# ROC    Relative Operating Characteristic\n# KAPPA (HSS)  Cohen's Kappa (Heidke skill score)\n# TSS (HK, PSS)    True skill statistic (Hanssen and Kuipers discriminant, Peirce's skill score)\n# FAR    False alarm ratio\n# SR     Success ratio\n# ACCURACY Accuracy (fraction correct)\n# BIAS   Bias score (frequency bias)\n# POD    Probability of detection (hit rate)\n# CSI    Critical success index (threat score)\n# ETS    Equitable threat score (Gilbert skill score)\n# POFD   Probability of false detection (false alarm rate)\n# OR     Odds ratio\n# ORSS   Odds ratio skill score (Yule's Q)\n# unsupported?\n# http://www.cawcr.gov.au/projects/verification/#Methods_for_dichotomous_forecasts\n# BOYCE .. not implemented?\n# TS       Threat score (critical success index)\n\n# function to save evaluate output\nbccvl.saveModelEvaluation <- function(out.model, out.biomod.model) {\n    # save the 'dismo::ModelEvalution' object\n    bccvl.save(out.model, name=\"dismo.eval.object.RData\")\n    # save all the model accuracy statistics provided in both dismo and biomod2\n    rownames(out.biomod.model) <- c(\"Testing.data\", \"Cutoff\", \"Sensitivity\", \"Specificity\")\n    bccvl.write.csv(t(round(out.biomod.model, digits=3)), name=\"combined.modelEvaluation.csv\")\n    # EMG no guarantee these value are correct\n\n    # save AUROC curve\n    png(file=file.path(bccvl.env$outputdir, 'AUC.png'))\n    plot(out.model, 'ROC');\n    dev.off()\n}\n\n#\n# returns:\n#\n#    best.iter: the best score obtained for chosen statistic\n#    cutoff: the associated cut-off used for transform fitted vector into binary\n#    sensibility: the sensibility with this threshold\n#    specificity: the specificity with this threshold\n#\n# Note this function looks to be based on biomod2's Find.Optim.Stat function - see that\n# for the original reference\n#\nbccvl.Find.Optim.Stat <- function(Stat='TSS', Fit, Obs, Precision=5, Fixed.thresh=NULL) {\n    uniform_obs=length(unique(Obs)) == 1\n    uniform_fit=length(unique(Fit)) == 1\n    if( uniform_obs | uniform_fit ) {\n        msg=sprintf(\"Stat: %s.\", Stat)\n        if (uniform_obs) msg=sprintf(\"%s Uniform observed data.\", msg)\n        if (uniform_fit) msg=sprintf(\"%s Uniform fitted data.\", msg)\n        msg=sprintf(\"%s Be careful with this model's predictions.\", msg)\n        # warning(\"\\nObserved or fited data contains only a value.. Evaluation Methods switched off\\n\",immediate.=T)\n        # best.stat <- cutoff <- true.pos <- sensibility <- true.neg <- specificity <- NA\n        warning(sprintf(\"\\n%s\\n\", msg),immediate.=T)\n        #best.stat <- cutoff <- true.pos <- sensibility <- true.neg <- specificity <- NA\n    } #else {\n    if(Stat != 'ROC'){\n        StatOptimum <- bccvl.getStatOptimValue(Stat)\n        if(is.null(Fixed.thresh)){ # test a range of threshold to get the one giving the best score\n            if(length(unique(Fit)) == 1){\n                valToTest <- unique(Fit)\n                valToTest <- round(c(mean(c(0,valToTest)), mean(c(1000,valToTest))))\n            } else{\n                mini <- max(min(quantile(Fit,0.05, na.rm=T), na.rm=T),0)\n                maxi <- min(max(quantile(Fit,0.95, na.rm=T), na.rm=T),1000)\n                # valToTest <- unique( round(c(seq(mini,maxi,length.out=100), mini, maxi)) )\n                # EMG no idea why the round() is here, it makes vals between 0 and 1 (ie bioclim) all 0\n                valToTest <- unique( c(seq(mini,maxi,length.out=100)))\n                # deal with unique value to test case\n                if(length(valToTest)<3){\n                    valToTest <- round(c(mean(0,mini), valToTest, mean(1000,maxi)))\n                }\n            }\n            # valToTest <- unique( c(seq(mini,maxi,by=Precision), mini, maxi) )\n        } else{\n            valToTest <- Fixed.thresh\n        }\n\n        calcStat <- sapply(lapply(valToTest, function(x){return(table(Fit>x,Obs))} ), bccvl.calculate.stat, stat=Stat)\n\n        # scal on 0-1 ladder.. 1 is the best\n        calcStat <- 1 - abs(StatOptimum - calcStat)\n\n        best.stat <- max(calcStat, na.rm=T)\n\n        cutoff <- median(valToTest[which(calcStat==best.stat)]) # if several values are selected\n\n        misc <- table(Fit >= cutoff, Obs)\n        misc <- bccvl.contagency.table.check(misc)\n        true.pos <- misc['TRUE','1']\n        true.neg <- misc['FALSE','0']\n        specificity <- (true.neg * 100)/sum(misc[,'0'])\n        sensibility <- (true.pos * 100)/sum(misc[,'1'])\n    } else{\n        roc1 <- roc(Obs, Fit, percent=T)\n        roc1.out <- coords(roc1, \"best\", ret=c(\"threshold\", \"sens\", \"spec\"))\n        best.stat <- as.numeric(auc(roc1))/100\n        cutoff <- as.numeric(roc1.out[\"threshold\"])\n        sensibility <- as.numeric(roc1.out[\"sensitivity\"])\n        specificity <- as.numeric(roc1.out[\"specificity\"])\n    }\n  #}\n    return(cbind(best.stat,cutoff,sensibility,specificity))\n}\n\nbccvl.getStatOptimValue <- function(stat) {\n    if(stat == 'TSS') return(1)\n    if(stat == 'KAPPA') return(1)\n    if(stat == 'ACCURACY') return(1)\n    if(stat == 'BIAS') return(1)\n    if(stat == 'POD') return(1)\n    if(stat == 'FAR') return(0)\n    if(stat == 'POFD') return(0)\n    if(stat == 'SR') return(1)\n    if(stat == 'CSI') return(1)\n    if(stat == 'ETS') return(1)\n    if(stat == 'HK') return(1)\n    if(stat == 'HSS') return(1)\n    if(stat == 'OR') return(1000000)\n    if(stat == 'ORSS') return(1)\n\n    #dismo\n    if(stat == 'ODP') return(1)\n    # if(stat == 'CCR') return(1) # same as ACCURACY\n    # if(stat == 'TPR') return(1) # same as POD\n    if(stat == 'TNR') return(1)\n    if(stat == 'FPR') return(0)\n    if(stat == 'FNR') return(0)\n    # if(stat == 'PPP') return(1) # same as SR\n    if(stat == 'NPP') return(1)\n    if(stat == 'MCR') return(0)\n    if(stat == 'OR') return(1000000)\n    # if(stat == 'kappa') return(1) # same as KAPPA\n}\n\nbccvl.calculate.stat <- function(Misc, stat='TSS') {\n    # Contagency table checking\n    Misc <- bccvl.contagency.table.check(Misc)\n\n    # Defining Classification index\n    hits <- Misc['TRUE','1']\n    misses <- Misc['FALSE','1']\n    false_alarms <- Misc['TRUE','0']\n    correct_negatives <- Misc['FALSE','0']\n\n    total <- sum(Misc)\n    forecast_1 <- sum(Misc['TRUE',])\n    forecast_0 <- sum(Misc['FALSE',])\n    observed_1 <- sum(Misc[,'1'])\n    observed_0 <- sum(Misc[,'0'])\n\n    # Calculating choosen evaluating metric\n    if(stat=='TSS') {\n        return( (hits/(hits+misses)) + (correct_negatives/(false_alarms+correct_negatives)) -1 )\n    }\n\n    if(stat=='KAPPA') {\n        Po <- (1/total) * (hits + correct_negatives)\n        Pe <- ((1/total)^2) * ((forecast_1 * observed_1) + (forecast_0 * observed_0))\n        return( (Po - Pe) / (1-Pe) )\n    }\n\n    if(stat=='ACCURACY') {\n        return( (hits + correct_negatives) / total)\n    }\n\n    if(stat=='BIAS') {\n        return( (hits + false_alarms) / (hits + misses))\n    }\n\n    if(stat=='POD') {\n        return( hits / (hits + misses))\n    }\n\n    if(stat=='FAR') {\n        return(false_alarms/(hits+false_alarms))\n    }\n\n    if(stat=='POFD') {\n        return(false_alarms / (correct_negatives + false_alarms))\n    }\n\n    if(stat=='SR') {\n        return(hits / (hits + false_alarms))\n    }\n\n    if(stat=='CSI') {\n        return(hits/(hits+misses+false_alarms))\n    }\n\n    if(stat=='ETS') {\n        hits_rand <- ((hits+misses)*(hits+false_alarms)) / total\n        return( (hits-hits_rand) / (hits+misses+false_alarms-hits_rand))\n    }\n\n    # if(stat=='HK') {\n    # return((hits/(hits+misses)) - (false_alarms/(false_alarms + correct_negatives)))\n    # }\n\n    # if(stat=='HSS') {\n    # expected_correct_rand <- (1/total) * ( ((hits+misses)*(hits+false_alarms)) +\n    # ((correct_negatives + misses)*(correct_negatives+false_alarms)) )\n    # return((hits+correct_negatives-expected_correct_rand) / (total - expected_correct_rand))\n    # }\n\n    # if(stat=='OR') {\n    # return((hits*correct_negatives)/(misses*false_alarms))\n    # }\n\n    # if(stat=='ORSS') {\n    # return((hits*correct_negatives - misses*false_alarms) / (hits*correct_negatives + misses*false_alarms))\n    # }\n\n    # if(stat==\"BOYCE\") {\n    #\n    # }\n\n    #dismo\n    if(stat=='ODP') {\n        return((false_alarms + correct_negatives) / total)\n    }\n\n    # if(stat=='CCR') {\n    # return((hits + correct_negatives) / total)\n    # }\n\n    # if(stat=='TPR') {\n    # return(hits / (hits + misses))\n    # }\n\n    if(stat=='TNR') {\n        return(correct_negatives / (false_alarms + correct_negatives))\n    }\n\n    if(stat=='FPR') {\n        return(false_alarms / (false_alarms + correct_negatives))\n    }\n\n    if(stat=='FNR') {\n        return(misses / (hits + misses))\n    }\n\n    # if(stat=='PPP') {\n    # return(hits / (hits + false_alarms))\n    # }\n\n    if(stat=='NPP') {\n        return(correct_negatives / (misses + correct_negatives))\n    }\n\n    if(stat=='MCR') {\n        return((false_alarms + misses) / total)\n    }\n\n    if(stat=='OR') {\n        return((hits * correct_negatives) / (misses * false_alarms))\n    }\n\n    # if(stat=='kappa') {\n    # return(((hits + correct_negatives) - (((hits + misses)*(hits + false_alarms) + (false_alarms + correct_negatives)*(misses + correct_negatives)) / total)) /\n    # (total -(((hits + misses)*(hits + false_alarms) + (false_alarms + correct_negatives)*(misses + correct_negatives)) / total)))\n    # }\n}\n\nbccvl.contagency.table.check <- function(Misc) {\n    # Contagency table checking\n    if(dim(Misc)[1]==1) {\n        if(row.names(Misc)[1]==\"FALSE\") {\n            Misc <- rbind(Misc, c(0,0))\n            rownames(Misc) <- c('FALSE','TRUE')\n        } else {\n            a <- Misc\n            Misc <- c(0,0)\n            Misc <- rbind(Misc, a)\n            rownames(Misc) <- c('FALSE','TRUE')\n        }\n    }\n\n    if(ncol(Misc) != 2 | nrow(Misc) !=2 ) {\n        Misc = matrix(0, ncol=2, nrow=2, dimnames=list(c('FALSE','TRUE'), c('0','1')))\n    }\n\n    if((sum(colnames(Misc) %in% c('FALSE','TRUE','0','1')) < 2) | (sum(rownames(Misc) %in% c('FALSE','TRUE','0','1')) < 2) ){\n        stop(\"Unavailable contagency table given\")\n    }\n\n    if('0' %in% rownames(Misc)) rownames(Misc)[which(rownames(Misc)=='0')] <- 'FALSE'\n    if('1' %in% rownames(Misc)) rownames(Misc)[which(rownames(Misc)=='1')] <- 'TRUE'\n\n    return(Misc)\n}\n\n# function to generate marginal (mean) response curves for dismo models\n# i.e., hold all but one predictor variable to its mean value and recalculate model predictions\nbccvl.createMarginalResponseCurves <- function(out.model, model.name) {\n   # get the enviromental variables and values used to create the model\n    if (model.name == \"brt\") {\n        model.values = matrix(out.model$data$x, ncol=length(out.model$var.names))\n        env.vars = out.model$var.names\n    } else if (model.name %in% c(\"geoIDW\", \"voronoiHull\")) {\n        model.values = rbind(out.model@presence, out.model@absence)\n        env.vars = colnames(model.values)\n    } else {\n        model.values = out.model@presence\n        env.vars = colnames(model.values)\n    }\n\n    if (!(length(model.values)==0)) {\n\n        # create a matrix to hold average values for each environmental variable\n        mean.values = matrix(data = NA, nrow = 100, ncol = length(env.vars))\n        colnames(mean.values) = env.vars\n        # for each variable, populate the column with the mean value\n        for (i in 1:ncol(mean.values)) {\n            mean.values[,i] = rep(mean(model.values[,i], na.rm=TRUE), 100)\n        }\n\n        # allow each environmental variable to vary, keeping other variable values at average, and predict suitability\n        for (j in 1:ncol(mean.values)) {\n            range.values = seq(min(model.values[,j], na.rm=TRUE), max(model.values[,j], na.rm=TRUE), length.out=100)\n            temp.data = mean.values\n            temp.data[,j] = range.values\n            if (model.name == \"brt\") {\n                colnames(temp.data) = env.vars\n                new.predictions = predict(out.model, as.data.frame(temp.data), n.trees = out.model$gbm.call$best.trees, type = \"response\")\n            } else {\n                new.predictions = predict(out.model, temp.data)\n            }\n\n            # create separate file for each response curve\n            save.name = env.vars[j]\n            png(file=file.path(bccvl.env$outputdir, paste(save.name, \"_response.png\", sep=\"\")))\n            plot(range.values, new.predictions, ylim=c(0,1), xlab=\"\", ylab=\"\", main=save.name, type=\"l\")\n            rug(model.values[,j])\n            dev.off()\n        }\n    } else {\n        write(paste(species, \": Cannot create response curves from\", model.name, \"object\", sep=\" \"), stdout())\n    }\n}\n\n# function to calculate variable importance values for dismo models based on biomod2's correlation between predictions\n# i.e., hold all but one predictor variable to its actual values, resample that one predictor and recalculate model predictions\nbccvl.calculateVariableImpt <- function(out.model, model.name, num_samples) {\n    # EMG num_samples should be same as biomod.VarImport arg set in\n    # 01.init.args.model.current.R\n\n    # get the enviromental variables and values used to create the model\n    # EMG this is duplicated from above, should be able to combine\n    if (model.name == \"brt\") {\n        model.values = matrix(out.model$data$x, ncol=length(out.model$var.names))\n        env.vars = out.model$var.names\n        colnames(model.values) = env.vars\n    } else if (model.name %in% c(\"geoIDW\", \"voronoiHull\")) {\n        model.values = rbind(out.model@presence, out.model@absence)\n        env.vars = colnames(model.values)\n    } else {\n        model.values = out.model@presence\n        env.vars = colnames(model.values)\n    }\n\n    if (!(length(model.values)==0)) {\n        # predict using actual values\n        if (model.name == \"brt\") {\n            actual.predictions = predict(out.model, as.data.frame(model.values), n.trees = out.model$gbm.call$best.trees, type = \"response\")\n        } else {\n            actual.predictions = predict(out.model, model.values)\n        }\n        # create a table to hold the output\n        varimpt.out = matrix(NA, nrow=length(env.vars), ncol=num_samples+2)\n        dimnames(varimpt.out) = list(env.vars, c(paste(\"sample_\", c(1:num_samples, \"mean\")), \"percent\"))\n        # create a copy of the env data matrix\n        sample.data = model.values\n        # for each predictor variable\n        for (p in 1:ncol(sample.data)) {\n            # for each num_sample\n            for (s in 1:num_samples) {\n                # resample from that variables' values, keeping other variable values the same, and predict suitability\n                sample.data[,p] = sample(x=sample.data[,p], replace=FALSE)\n                # predict using sampled values\n                if (model.name == \"brt\") {\n                    new.predictions = predict(out.model, as.data.frame(sample.data), n.trees = out.model$gbm.call$best.trees, type = \"response\")\n                } else {\n                    new.predictions = predict(out.model, sample.data)\n                }\n                # calculate correlation between original predictions and new predictions\n                varimpt.out[p,s] = 1-max(round(cor(x=actual.predictions, y=new.predictions, use=\"pairwise.complete.obs\", method=\"pearson\"), digits=3),0)\n            }\n        }\n        # calculate mean variable importance, normalize to percentages, and write results\n        varimpt.out[,num_samples+1] = round(rowMeans(varimpt.out, na.rm=TRUE), digits=3)\n        varimpt.out[,num_samples+2] = round((varimpt.out[,num_samples+1]/sum(varimpt.out[,num_samples+1]))*100, digits=0)\n        bccvl.write.csv(varimpt.out, name=\"biomod2_like_VariableImportance.csv\")\n    } else {\n        write(paste(species, \": Cannot calculate variable importance for \", model.name, \"object\", sep=\" \"), stdout())\n    }\n}\n\n# function to calculate variable importance values for dismo models based on Maxent's decrease in AUC\n# i.e., hold all but one predictor variable to its original values, resample that one predictor and recalculate model AUC\nbccvl.calculatePermutationVarImpt <- function(out.model, model.eval,\n                                              model.name, occur, bkgd) {\n    # get the enviromental variables and values used to create the model\n    # EMG this is duplicated from above, should be able to combine or find an easier way to determine\n    if (model.name == \"brt\") {\n        model.values = matrix(out.model$data$x, ncol=length(out.model$var.names))\n        env.vars = out.model$var.names\n        colnames(model.values) = env.vars\n    } else if (model.name %in% c(\"geoIDW\", \"voronoiHull\")) {\n        model.values = rbind(out.model@presence, out.model@absence)\n        env.vars = colnames(model.values)\n    } else {\n        model.values = out.model@presence\n        env.vars = colnames(model.values)\n    }\n\n    if (!(length(model.values)==0)) {\n        # get the occurrence and background environmental data used to evaluate the model\n        p.swd=occur\n        a.swd=bkgd\n        # get the AUC from the original model evaluation\n        init.auc = round(model.eval@auc, digits=3)\n        # create a table to hold the output\n        permvarimpt.out = matrix(NA, nrow=length(env.vars), ncol=4)\n        dimnames(permvarimpt.out) = list(env.vars, c(\"init.auc\", \"sample.auc\", \"change.auc\", \"percent\"))\n        permvarimpt.out[,\"init.auc\"] = rep(init.auc, length(env.vars))\n        # create a copy of the occurrence and background environmental data\n        sample.p = p.swd[,env.vars, drop=FALSE]\n        sample.a = a.swd[,env.vars, drop=FALSE]\n\t\t# check for and remove any NA's present in the data\n\t\tno.na.sample.p = na.omit(sample.p);\n                no.na.sample.a = na.omit(sample.a)\n\t\tif (nrow(no.na.sample.p) != nrow(sample.p)) {\n\t\t\twrite(paste(\"bccvl.calculatePermutationVarImpt(): NA's were removed from presence data!\"), stdout())\n\t\t}\n\t\tif (nrow(no.na.sample.a) != nrow(sample.a)) {\n\t\t\twrite(paste(\"bccvl.calculatePermutationVarImpt(): NA's were removed from absence data!\"), stdout())\n\t\t}\n        # for each predictor variable\n        for (v in 1:length(env.vars)) {\n\t\t\t# resample from that variables' values, keeping other variable values the same\n\t\t\tno.na.sample.p[,v] = sample(x=no.na.sample.p[,v], replace=FALSE)\n\t\t\tno.na.sample.a[,v] = sample(x=no.na.sample.a[,v], replace=FALSE)\n            # re-evaluate model with sampled env values\n            if (model.name == \"brt\") {\n                sample.eval = dismo::evaluate(p=no.na.sample.p, a=no.na.sample.a, model=out.model, n.trees=out.model$gbm.call$best.trees)\n            } else {\n                sample.eval = dismo::evaluate(p=no.na.sample.p, a=no.na.sample.a, model=out.model)\n            }\n            # get the new auc\n            permvarimpt.out[v,\"sample.auc\"] = round(sample.eval@auc, digits=3)\n        }\n        # calculate the difference in auc, normalize to percentages, and write results\n        permvarimpt.out[,\"change.auc\"] = permvarimpt.out[,\"init.auc\"] - permvarimpt.out[,\"sample.auc\"]\n        for (r in 1:nrow(permvarimpt.out)) {\n            if (permvarimpt.out[r,\"change.auc\"] < 0) {  # EMG what if AUC increases?\n                permvarimpt.out[r,\"change.auc\"] = 0\n            }\n        }\n        permvarimpt.out[,\"percent\"] = round((permvarimpt.out[,\"change.auc\"]/sum(permvarimpt.out[,\"change.auc\"]))*100, digits=0)\n        bccvl.write.csv(permvarimpt.out, name=\"maxent_like_VariableImportance.csv\")\n    } else {\n        write(paste(species, \": Cannot calculate maxent-like variable importance for \", model.name, \"object\", sep=\" \"), stdout())\n    }\n}\n\n# function to create HTML file with accuracy measures\n# need to install and read in the following packages:\n#install.packages(c(\"R2HTML\", \"png\"))\n#library(R2HTML)\n#library(png)\nbccvl.generateHTML <- function() {\n\n    # read in model outputs\n    auccurve = readPNG(file.path(bccvl.env$outputdir, \"AUC.png\"))\n    accuracystats <- read.csv(file.path(bccvl.env$outputdir, \"combined.modelEvaluation.csv\"),\n                              row.names=c(1))\n\n    # create the output file\n    target = HTMLInitFile(outdir=bccvl.env$outputdir, filename=\"results\", BackGroundColor=\"#CCCCCC\")\n\n    # add content\n    HTML(\"<center><br><H1>Model Output for \", file=target)\n\n    HTML(\"<br><H2>AUC:ROC curve\", file=target)\n    HTMLInsertGraph(\"AUC.png\", file=target)\n\n    HTML(\"<br><H2>Accuracy measures\",file=target)\n    HTML(accuracystats, file=target)\n\n    # close the file\n    HTMLEndFile()\n}\n\n###############\n#\n# evaluate(p, a, model, x, tr, ...)\n#\n# p presence points (x and y coordinate or SpatialPoints* object)\n# Or, if x is missing, values at presence points (EMG: values returned by a predict())\n# Or, a matrix with values to compute predictions for\n# a absence points (x and y coordinate or SpatialPoints* object)\n# Or, if x is missing, values at absence points (EMG: values returned by a predict())\n# Or, a matrix with values to compute predictions for\n# model any fitted model, including objects inheriting from 'DistModel'; not used when x is missing\n# x Optional. Predictor values (object of class Raster*). If present, p and a are interpreted\n# as (spatial) points (EMG: lon/lat)\n# tr Optional. a vector of threshold values to use for computing the confusion matrices\n# ... Additional arguments for the predict function (EMG: evaluate() calls predict())\n#\n# 'ModelEvaluation' output based on Fielding and Bell (1997) with attributes:\n# presence - presence data used\n# absence - absence data used\n# np - number of presence points\n# na - number of absence points\n# auc - Area under the receiver operator (ROC) curve\n# pauc - p-value for the AUC (for the Wilcoxon test W statistic\n# cor - Correlation coefficient\n# pcor - p-value for correlation coefficient\n# t - vector of thresholds used to compute confusion matrices\n# confusion - confusion matrices\n# prevalence - Prevalence\n# ODP - Overall diagnostic power\n# CCR - Correct classification rate\n# TPR - True positive rate\n# TNR - True negative rate\n# FPR - False positive rate\n# FNR - False negative rate\n# PPP - Positive predictive power\n# NPP - Negative predictive power\n# MCR - Misclassification rate\n# OR - Odds-ratio\n# kappa - Cohen's kappa\n#\n###############\n\n###evaluate the models and save the outputs\nbccvl.evaluate.model <- function(model.name, model.obj, occur, bkgd) {\n    # evaluate model using dismo's evaluate\n    if (model.name == \"brt\") {\n        model.eval = dismo::evaluate(p=occur, a=bkgd, model=model.obj, n.trees=model.obj$gbm.call$best.trees)\n    } else {\n        model.eval = dismo::evaluate(p=occur, a=bkgd, model=model.obj)\n    }\n    # need predictions and observed values to create confusion matrices for accuracy statistics\n    model.fit = c(model.eval@presence, model.eval@absence)\n    model.obs = c(rep(1, length(model.eval@presence)), rep(0, length(model.eval@absence)))\n\n    # get the model accuracy statistics using a modified version of biomod2's Evaluate.models.R\n    # TODO: model.accuracy is another global variable\n    model.combined.eval = sapply(model.accuracy, function(x){\n        return(bccvl.Find.Optim.Stat(Stat=x, Fit=model.fit, Obs=model.obs))\n    })\n    # save output\n    bccvl.saveModelEvaluation(model.eval, model.combined.eval)\n\n    # create response curves\n    bccvl.createMarginalResponseCurves(model.obj, model.name)\n\n    # calculate variable importance (like biomod2, using correlations between predictions)\n    bccvl.calculateVariableImpt(model.obj, model.name, 3)\n\n    # calculate variable importance (like maxent, using decrease in AUC)\n    bccvl.calculatePermutationVarImpt(model.obj, model.eval, model.name, occur, bkgd)\n\n    # create HTML file with accuracy measures\n    bccvl.generateHTML()\n} # end of evaluate.modol\n\n\n# function to save evaluate output for BIOMOD2 models\nbccvl.saveBIOMODModelEvaluation <- function(loaded.names, biomod.model) {\n    # get and save the model evaluation statistics\n    # EMG these must specified during model creation with the arg \"models.eval.meth\"\n    evaluation = get_evaluations(biomod.model)\n    bccvl.write.csv(evaluation, name=\"biomod2.modelEvaluation.csv\")\n\n    # get the model predictions and observed values\n    predictions = getModelsPrediction(biomod.model)\n    total_models = length(dimnames(predictions)[[3]])\n\n    # TODO: get_predictions is buggy; evaluation=FALSE works the wrong way round\n    # predictions = get_predictions(biomod.model, evaluation=FALSE)\n    obs = get_formal_data(biomod.model, \"resp.var\")\n    # in case of pseudo absences we might have NA values in obs so replace them with 0\n    obs = replace(obs, is.na(obs), 0)\n\n    for ( i in 1:total_models )\n    {\n        model_name = dimnames(predictions)[[3]][i]  # will be FULL or RUN1 for eg\n        model_predictions = predictions[,,i,,drop=FALSE]\n\n        if (sum(is.na(model_predictions)) == length(model_predictions)) \n        {\n            # somewhat adhoc method of determining that the model failed to predict anything\n            # Note that we can determine the computed and failed models by inspecting \n            # biomod.model@models.computed and biomod.model@models.failed respectively, however\n            # dimnames (model_name) can't be used to match these in a straight forward\n            # manner (it may be possible, could go either way here. this way feels simpler)\n\n            # Warn that model n is being ignored. It most probably failed to build.\n            warning(sprintf(\"Warning: Model %i failed to generate. Not generating stats\", i), immediate.=T)\n            next\n        }\n        # get the model accuracy statistics using a modified version of biomod2's Evaluate.models.R\n        # TODO: model.accuracy is another global variable\n        combined.eval = sapply(model.accuracy, function(x){\n            return(bccvl.Find.Optim.Stat(Stat = x, Fit = model_predictions, Obs = obs))\n        })\n        # save all the model accuracy statistics provided in both dismo and biomod2\n        rownames(combined.eval) <- c(\"Testing.data\",\"Cutoff\",\"Sensitivity\", \"Specificity\")\n        #bccvl.write.csv(t(round(combined.eval, digits=3)), name=\"combined.modelEvaluation.csv\")\n        bccvl.write.csv(t(round(combined.eval, digits=3)), name=paste(\"combined\", model_name, \"modelEvaluation.csv\", sep=\".\"))\n\n        # save AUC curve\n        require(pROC, quietly=T)\n        roc1 <- roc(as.numeric(obs), as.numeric(model_predictions), percent=T)\n        png(file=file.path(bccvl.env$outputdir, paste(\"pROC\", model_name, \"png\", sep=\".\")))\n        plot(roc1, main=paste(\"AUC=\",round(auc(roc1)/100,3),sep=\"\"), legacy.axes=TRUE)\n        dev.off()\n        # save occurence/absence pdfs\n        occur_vals=data.frame(vals=roc1$predictor[roc1$response==1])\n        absen_vals=data.frame(vals=roc1$predictor[roc1$response==0])\n        absen_vals$label=\"absent\"\n        occur_vals$label=\"occur\"\n        vals=rbind(occur_vals,absen_vals)\n        myplot=ggplot(vals, aes(vals, fill=label))  + geom_density(alpha = 0.3)\n        myplot = myplot + ggtitle(\"Occurrence/absence probability density functions\\nbased on model predicted value\")\n        ggsave(myplot, filename=paste(\"occurence_absence_pdf\", model_name, \"png\", sep=\".\"), scale=1.0)\n        dev.off()\n\n        myplot=ggplot(vals, aes(vals, fill=label))  + geom_histogram(alpha = 0.5)\n        myplot = myplot + ggtitle(\"Occurrence/absence histograms\\nbased on model predicted value\")\n        ggsave(myplot, filename=paste(\"occurence_absence_hist\", model_name, \"png\", sep=\".\"), scale=1.0)\n        dev.off()\n\n        png(file=file.path(bccvl.env$outputdir, paste(\"true_and_false_posivite_rates\", model_name, \"png\", sep=\".\")))\n        plot(roc1$thresholds, 100-roc1$specificities, type=\"p\", col=\"blue\", xlab=\"Classification threshold\", ylab=\"Rate\")\n        par(new=TRUE)\n        plot(roc1$thresholds, roc1$sensitivities, type=\"p\", col=\"red\", xlab=\"\", ylab=\"\")\n        legend(\"topright\",  title='', legend=c(\"True positive rate\", \"False positive rate\"), fill=c(\"red\", \"blue\"), horiz=TRUE)\n        title(\"True and false positive rates according to\\nclassification threshold\")\n        dev.off()\n\n\n        # get and save the variable importance estimates\n        variableImpt = get_variables_importance(biomod.model)\n        if (!is.na(variableImpt)) {\n        #EMG Note this will throw a warning message if variables (array) are returned\n            bccvl.write.csv(variableImpt, name=paste(\"variableImportance\", model_name, \"txt\", sep=\".\"))\n        } else {\n            message(\"VarImport argument not specified during model creation!\")\n            #EMG must create the model with the arg \"VarImport\" != 0\n        }\n    }\n\n    # save response curves (Elith et al 2005)\n    # TODO: check models parameter ... do I need it? shouldn't it be algo name?\n    #       -> would make BIOMOD_LoadMadels call and parameter loaded.name pointless\n    #\n    # not sure what the comment above means - but ever since we moved to generating\n    # output from all models, we could just use biomod.model@models.computed\n    for(name in loaded.names)\n    {\n\n        png(file=file.path(bccvl.env$outputdir, sprintf(\"mean_response_curves_%s.png\", name)))\n        test <- response.plot2(models = name,\n                               Data = get_formal_data(biomod.model,\"expl.var\"),\n                               show.variables = get_formal_data(biomod.model,\"expl.var.names\"),\n                               fixed.var.metric = \"mean\")\n         #, data_species = getModelsInputData(biomod.model,\"resp.var\"))\n         # EMG need to investigate why you would want to use this option - uses presence data only\n        dev.off()\n    }\n}\n\n####\n##\n##  INPUT:\n##\n##  occur.data ... filename for occurence data\n##  absen.data  ... filename for absence data\n##  enviro.data.current ... list of filenames for climate data\n##  enviro.data.type    ... continuous\n##  opt.tails ... predict parameter\n##\n##  outputdir ... root folder for output data\n\n#define the working directory\n#scriptdir = normalizePath(bccvl.env$scriptdir)\n#inputdir =  normalizePath(bccvl.env$inputdir)\n#outputdir =  normalizePath(bccvl.env$outputdir)\n\n\n# extract params\n# define the lon/lat of the observation records -- 2 column matrix of longitude and latitude\noccur.data = bccvl.params$species_occurrence_dataset$filename\noccur.species = bccvl.params$species_occurrence_dataset$species\n#define the the lon/lat of the background / psuedo absence points to use -- 2 column matrix of longitude and latitude\nabsen.data = bccvl.params$species_absence_dataset$filename\n#define the current enviro data to use\nenviro.data.current = lapply(bccvl.params$environmental_datasets, function(x) x$filename)\n#type in terms of continuous or categorical\nenviro.data.type = lapply(bccvl.params$environmental_datasets, function(x) x$type)\n\n\n############### BIOMOD2 Models ###############\n#\n# general parameters to perform any biomod modelling\n#\nbiomod.NbRunEval = bccvl.params$nb_run_eval  # default 10; n-fold cross-validation; ignored if DataSplitTable is filled\nbiomod.DataSplit = bccvl.params$data_split # default 100; % for calibrating/training, remainder for testing; ignored if DataSplitTable is filled\nbiomod.Yweights = NULL #response points weights\nbiomod.Prevalence = bccvl.params$prevalence #either NULL (default) or a 0-1 numeric used to build \"weighted response weights\"\nbiomod.VarImport = bccvl.params$var_import # default 0; number of resampling of each explanatory variable to measure the relative importance of each variable for each selected model\n#EMG this parameter needs to be specified in order to get VariableImportance metrics during model evaluation\nbiomod.models.eval.meth = c(\"KAPPA\", \"TSS\", \"ROC\" ,\"FAR\", \"SR\", \"ACCURACY\", \"BIAS\", \"POD\", \"CSI\", \"ETS\") #vector of evaluation metrics\nbiomod.rescal.all.models = bccvl.params$rescale_all_models #if true, all model prediction will be scaled with a binomial GLM\nbiomod.do.full.models = bccvl.params$do_full_models #if true, models calibrated and evaluated with the whole dataset are done; ignored if DataSplitTable is filled\nbiomod.modeling.id = bccvl.params$modeling_id #character, the ID (=name) of modeling procedure. A random number by default\n# biomod.DataSplitTable = NULL #a matrix, data.frame or a 3D array filled with TRUE/FALSE to specify which part of data must be used for models calibration (TRUE) and for models validation (FALSE). Each column correspund to a \"RUN\". If filled, args NbRunEval, DataSplit and do.full.models will be ignored\n# EMG Need to test whether a NULL values counts as an argument\nbiomod.species.name = occur.species # used for various path and file name generation\nprojection.name = \"current\"  #basename(enviro.data.current)\n\n\n# model-specific arguments to create a biomod model\nmodel.options.ann <- list(\n\tNbCV = bccvl.params$nbcv, #nb of cross validation to find best size and decay parameters\n\trang = bccvl.params$rang, #Initial random weights on [-rang, rang]\n\tmaxit = bccvl.params$maxit #maximum number of iterations. Default 100\n)\n\n############### BIOMOD2 Models ###############\n#\n# general parameters to project any biomod modelling\n#\n#modeling.output #\"BIOMOD.models.out\" object produced by a BIOMOD_Modeling run\n#new.env #a set of explanatory variables onto which models will be projected; must match variable names used to build the models\n#proj.name #a character defining the projection name (a new folder will be created with this name)\n# pseudo absences\nbiomod.PA.nb.rep = 0\nbiomod.PA.nb.absences = 0\n\nbiomod.xy.new.env = NULL #optional coordinates of new.env data. Ignored if new.env is a rasterStack\nbiomod.selected.models = bccvl.params$selected_models #'all' when all models have to be used to render projections or a subset vector of modeling.output models computed (eg, = grep('_RF', getModelsBuiltModels(myBiomodModelOut)))\n# EMG If running one model at a time, this parameter becomes irrevelant\nbiomod.binary.meth = NULL #a vector of a subset of models evaluation method computed in model creation\nbiomod.filtered.meth = NULL #a vector of a subset of models evaluation method computed in model creation\nbiomod.compress = bccvl.params$compress # default 'gzip'; compression format of objects stored on your hard drive. May be one of `xz', `gzip' or NULL\nbiomod.build.clamping.mask = TRUE #if TRUE, a clamping mask will be saved on hard drive\nopt.biomod.silent = FALSE #logical, if TRUE, console outputs are turned off\nopt.biomod.do.stack = TRUE #logical, if TRUE, attempt to save all projections in a unique object i.e RasterStack\nopt.biomod.keep.in.memory = TRUE #logical, if FALSE only the link pointing to a hard drive copy of projections are stored in output object\nopt.biomod.output.format = NULL #'.Rdata', '.grd' or '.img'; if NULL, and new.env is not a Raster class, output is .RData defining projections saving format (on hard drive)\n\n\n# model accuracy statistics\n# these are available from dismo::evaluate.R NOT originally implemented in biomod2::Evaluate.models.R\ndismo.eval.method = c(\"ODP\", \"TNR\", \"FPR\", \"FNR\", \"NPP\", \"MCR\", \"OR\")\n# and vice versa\n\n# model accuracy statistics - combine stats from dismo and biomod2 for consistent output\nmodel.accuracy = c(dismo.eval.method, biomod.models.eval.meth)\n# TODO: these functions are used to evaluate the model ... configurable?\n\n# read current climate data\n# TODO: check env.data for spatial reference\ncurrent.climate.scenario = bccvl.enviro.stack(enviro.data.current)\n\n###read in the necessary observation, background and environmental data\noccur = bccvl.species.read(occur.data) #read in the observation data lon/lat\n# keep only lon and lat columns\noccur = occur[c(\"lon\",\"lat\")]\n\n# shall we use pseudo absences?\n# TODO: this will ignore given absence file in case we want pseudo absences\nif (bccvl.params$species_pseudo_absence_points) {\n    biomod.PA.nb.rep = 1\n    biomod.PA.nb.absences = bccvl.params$species_number_pseudo_absence_points\n    # create an empty data frame for bkgd points\n    absen = data.frame(lon=numeric(0), lat=numeric(0))\n} else {\n    # read absence points from file\n    absen = bccvl.species.read(absen.data) #read in the background position data lon.lat\n    # keep only lon and lat columns\n    absen = absen[c(\"lon\",\"lat\")]\n}\n\n\n# extract enviro data for species observation points and append to species data\n#occur = cbind(occur, extract(current.climate.scenario, cbind(occur$lon, occur$lat)))\n#if (!is.null(absen)) {\n#    absen = cbind(absen, extract(current.climate.scenario, cbind(absen$lon, absen$lat)))\n#}\n\n###run the models and store models\n############### BIOMOD2 Models ###############\n# 1. Format the data\n# 2. Define the model options\n# 3. Compute the model\n# NOTE: Model evaluation is included as part of model creation\n\n# BIOMOD_FormatingData(resp.var, expl.var, resp.xy = NULL, resp.name = NULL, eval.resp.var = NULL,\n#\teval.expl.var = NULL, eval.resp.xy = NULL, PA.nb.rep = 0, PA.nb.absences = 1000, PA.strategy = 'random',\n#\tPA.dist.min = 0, PA.dist.max = NULL, PA.sre.quant = 0.025, PA.table = NULL, na.rm = TRUE)\n#\n# resp.var a vector, SpatialPointsDataFrame (or SpatialPoints if you work with `only presences' data) containing species data (a single species) in binary format (ones for presences, zeros for true absences and NA for indeterminated ) that will be used to build the species distribution models.\n# expl.var a matrix, data.frame, SpatialPointsDataFrame or RasterStack containing your explanatory variables that will be used to build your models.\n# resp.xy optional 2 columns matrix containing the X and Y coordinates of resp.var (only consider if resp.var is a vector) that will be used to build your models.\n# eval.resp.var\ta vector, SpatialPointsDataFrame your species data (a single species) in binary format (ones for presences, zeros for true absences and NA for indeterminated ) that will be used to evaluate the models with independant data (or past data for instance).\n# eval.expl.var\ta matrix, data.frame, SpatialPointsDataFrame or RasterStack containing your explanatory variables that will be used to evaluate the models with independant data (or past data for instance).\n# eval.resp.xy opional 2 columns matrix containing the X and Y coordinates of resp.var (only consider if resp.var is a vector) that will be used to evaluate the modelswith independant data (or past data for instance).\n# resp.name\tresponse variable name (character). The species name.\n# PA.nb.rep\tnumber of required Pseudo Absences selection (if needed). 0 by Default.\n# PA.nb.absences number of pseudo-absence selected for each repetition (when PA.nb.rep > 0) of the selection (true absences included)\n# PA.strategy strategy for selecting the Pseudo Absences (must be `random', `sre', `disk' or `user.defined')\n# PA.dist.min minimal distance to presences for `disk' Pseudo Absences selection (in meters if the explanatory is a not projected raster (+proj=longlat) and in map units (typically also meters) when it is projected or when explanatory variables are stored within table )\n# PA.dist.max maximal distance to presences for `disk' Pseudo Absences selection(in meters if the explanatory is a not projected raster (+proj=longlat) and in map units (typically also meters) when it is projected or when explanatory variables are stored within table )\n# PA.sre.quant quantile used for `sre' Pseudo Absences selection\n# PA.table a matrix (or a data.frame) having as many rows than resp.var values. Each column correspund to a Pseudo-absences selection. It contains TRUE or FALSE indicating which values of resp.var will be considered to build models. It must be used with `user.defined' PA.strategy.\n# na.rm\tlogical, if TRUE, all points having one or several missing value for environmental data will be removed from analysis\n\n# format the data as required by the biomod package\nformatBiomodData <- function() {\n    biomod.data <- rbind(occur[,c(\"lon\", \"lat\")], absen[,c(\"lon\", \"lat\")])\n    biomod.data.pa <- c(rep(1, nrow(occur)), rep(0, nrow(absen)))\n    myBiomodData <-\n        BIOMOD_FormatingData(resp.var  = biomod.data.pa,\n                             expl.var  = current.climate.scenario,\n                             resp.xy   = biomod.data,\n                             resp.name = biomod.species.name,\n                             PA.nb.rep = biomod.PA.nb.rep,\n                             PA.nb.absences = biomod.PA.nb.absences,\n                             PA.strategy = 'random')\n    return(myBiomodData)\n}\n\n# BIOMOD_Modeling(data, models = c('GLM','GBM','GAM','CTA','ANN','SRE','FDA','MARS','RF','MAXENT'), models.options = NULL,\n#\tNbRunEval=1, DataSplit=100, Yweights=NULL, Prevalence=NULL, VarImport=0, models.eval.meth = c('KAPPA','TSS','ROC'),\n#\tSaveObj = TRUE, rescal.all.models = TRUE, do.full.models = TRUE, modeling.id = as.character(format(Sys.time(), '%s')),\n#\t...)\n#\n# data\tBIOMOD.formated.data object returned by BIOMOD_FormatingData\n# models vector of models names choosen among 'GLM', 'GBM', 'GAM', 'CTA', 'ANN', 'SRE', 'FDA', 'MARS', 'RF' and 'MAXENT'\n# models.options BIOMOD.models.options object returned by BIOMOD_ModelingOptions\n# NbRunEval\tNumber of Evaluation run\n# DataSplit\t% of data used to calibrate the models, the remaining part will be used for testing\n# Yweights response points weights\n# Prevalence either NULL (default) or a 0-1 numeric used to build 'weighted response weights'\n# VarImport\tNumber of permutation to estimate variable importance\n# models.eval.meth vector of names of evaluation metric among 'KAPPA', 'TSS', 'ROC', 'FAR', 'SR', 'ACCURACY', 'BIAS', 'POD', 'CSI' and 'ETS'\n# SaveObj keep all results and outputs on hard drive or not (NOTE: strongly recommended)\n# rescal.all.models\tif true, all model prediction will be scaled with a binomial GLM\n# do.full.models if true, models calibrated and evaluated with the whole dataset are done\n# modeling.id character, the ID (=name) of modeling procedure. A random number by default.\n# ... further arguments :\n# DataSplitTable : a matrix, data.frame or a 3D array filled with TRUE/FALSE to specify which part of data must be used for models calibration (TRUE) and for models validation (FALSE). Each column correspund to a 'RUN'. If filled, args NbRunEval, DataSplit and do.full.models will be ignored.\n\n###############\n#\n# ANN - artificial neural network (nnet)\n#\n###############\n\n# myBiomodOptions <- BIOMOD_ModelingOptions(ANN = list(NbCV = 5, rang = 0.1, maxit = 200))\n# NbCV : nb of cross validation to find best size and decay parameters\n# rang : Initial random weights on [-rang, rang]\n# maxit : maximum number of iterations. Default 100\n\n# 1. Format the data\nmodel.data = formatBiomodData()\n# 2. Define the model options\nmodel.options <- BIOMOD_ModelingOptions(ANN = model.options.ann)\n# 3. Compute the model\nmodel.sdm <-\n    BIOMOD_Modeling(data              = model.data,\n                    models            = c('ANN'),\n                    models.options    = model.options,\n                    NbRunEval         = biomod.NbRunEval,\n                    DataSplit         = biomod.DataSplit,\n                    Yweights          = biomod.Yweights,\n                    Prevalence        = biomod.Prevalence,\n                    VarImport         = biomod.VarImport,\n                    models.eval.meth  = biomod.models.eval.meth,\n                    SaveObj           = TRUE,\n                    rescal.all.models = biomod.rescal.all.models,\n                    do.full.models    = biomod.do.full.models,\n                    modeling.id       = biomod.modeling.id\n                    )\n#save out the model object\n# TODO: biomod stores the model already in species/species.bccvl.models.out\n# TODO: get species name into this somehow -> requires archive generation on input to do the same\nbccvl.save(model.sdm, name=\"model.object.RData\")\n# predict for current climate scenario\n# TODO: would I want to use saveObj here again?\nmodel.proj <-\n    BIOMOD_Projection(modeling.output     = model.sdm,\n                      new.env             = current.climate.scenario,\n                      proj.name           = projection.name,  #basename(enviro.data.current), {{ species }}\n                      xy.new.env          = biomod.xy.new.env,\n                      selected.models     = biomod.selected.models,\n                      binary.meth         = biomod.binary.meth,\n                      filtered.meth       = biomod.filtered.meth,\n                      #compress            = biomod.compress,\n                      build.clamping.mask = biomod.build.clamping.mask,\n                      silent              = opt.biomod.silent,\n                      do.stack            = opt.biomod.do.stack,\n                      keep.in.memory      = opt.biomod.keep.in.memory,\n                      output.format       = opt.biomod.output.format)\n# convert projection output from grd to gtiff\n# TODO: get proj4string in here somewhere and use in grdtogtiff\nbccvl.grdtogtiff(file.path(getwd(),\n                           biomod.species.name,\n                           paste(\"proj\", projection.name, sep=\"_\")))\n\n# output is saved as part of the projection, format specified in arg 'opt.biomod.output.format'\n# evaluate model\nloaded.model = BIOMOD_LoadModels(model.sdm, models=\"ANN\") # load model\nbccvl.saveBIOMODModelEvaluation(loaded.model, model.sdm)\n"
        }, 
        "zipworkenv": true
    }
}
